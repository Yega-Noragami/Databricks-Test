{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d410ad8a-ca46-430c-b76b-042f85ba6f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTasks Needed to be done: \\n1. Data Ingestion\\n2. Data Transformation \\n3. Data Manipulation \\n4. Data Visualisation \\n5. Data Storage \\n\\n\\nBonus Task: \\n1. Repeat the data manipulation operations using Spark SQL using SQL Statements (e.g.:\\n“SELECT Salary FROM employees ....“\\n\\n2. Write down how would you execute the Data Storage steps storing the data to:\\no Azure CosmosDB\\no Parquet in Azure Storage\\n\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Analysis project using pyspark \n",
    "'''\n",
    "Tasks Needed to be done: \n",
    "1. Data Ingestion\n",
    "2. Data Transformation \n",
    "3. Data Manipulation \n",
    "4. Data Visualisation \n",
    "5. Data Storage \n",
    "\n",
    "\n",
    "Bonus Task: \n",
    "1. Repeat the data manipulation operations using Spark SQL using SQL Statements (e.g.:\n",
    "“SELECT Salary FROM employees ....“\n",
    "\n",
    "2. Write down how would you execute the Data Storage steps storing the data to:\n",
    "o Azure CosmosDB\n",
    "o Parquet in Azure Storage\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "794f8fc1-bd2e-40ee-8143-ac3b3625778e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, desc, rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15cfe33c-3845-4ce8-ac07-7d31436f51fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"EmployeeDataAnalysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0e34704-e964-4dd5-8cfe-4f6c8212b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+----------+----------+------+\n",
      "|EmployeeID|FirstName| LastName| BirthDate|Department|Salary|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "|         1|     John|      Doe|01/12/1980|     Sales| 70000|\n",
      "|         2|     Jane|    Smith|14/07/1985| Marketing| 80000|\n",
      "|         3|   Oliver|  Johnson|30/06/1990|        IT| 90000|\n",
      "|         4|     Emma| Williams|21/01/1989|        HR| 75000|\n",
      "|         5|     Liam|    Brown|05/03/1987|     Sales| 85000|\n",
      "|         6|      Ava|   Garcia|22/04/1995|      NULL| 82000|\n",
      "|         7|  William| Martinez|10/02/1981|        IT| 77000|\n",
      "|         8|   Sophia| Robinson|12/09/1988| Marketing| 94000|\n",
      "|         9|    James|    Clark|19/06/1982|      NULL| 81000|\n",
      "|        10|Charlotte|Rodriguez|08/07/1991|        HR| 88000|\n",
      "|        11| Benjamin|    Lewis|30/11/1983|     Sales| 95000|\n",
      "|        12|      Mia|      Lee|17/02/1986| Marketing| 78000|\n",
      "|        13|    Ethan|   Walker|25/08/1992|      NULL| 92000|\n",
      "|        14|   Harper|     Hall|31/05/1993|     Sales| 89000|\n",
      "|        15|     Noah|    Allen|12/01/1984|        IT| 76000|\n",
      "|        16|   Amelia|    Young|13/02/1994| Marketing| 93000|\n",
      "|        17|    Jacob|Hernandez|27/04/1987|     Sales| 91000|\n",
      "|        18| Isabella|     King|03/12/1992|      NULL| 74000|\n",
      "|        19|     Luke|   Wright|22/01/1985|        IT| 86000|\n",
      "|        20|   Evelyn|    Lopez|05/03/1990| Marketing| 83000|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------- DATA Ingestion ------------\n",
    "\n",
    "# First We are going to perform Data Ingestion \n",
    "\n",
    "# Set up the path to employees\n",
    "data_path = \"employees.csv\"\n",
    "\n",
    "# Load CSV data into a DataFrame\n",
    "employees_df = spark.read.csv(data_path, header=True ,inferSchema=True,  sep=';')\n",
    "\n",
    "# View Employees Table \n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9f6b8e8-efa4-4117-ab65-f3efaebf37da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- BirthDate: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial Data Screening \n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c412b25e-abec-47ab-ba31-97da616ff394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+----------+----------+------+\n",
      "|EmployeeID|FirstName| LastName| BirthDate|Department|Salary|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "|         1|     John|      Doe|01/12/1980|     Sales| 70000|\n",
      "|         2|     Jane|    Smith|14/07/1985| Marketing| 80000|\n",
      "|         3|   Oliver|  Johnson|30/06/1990|        IT| 90000|\n",
      "|         4|     Emma| Williams|21/01/1989|        HR| 75000|\n",
      "|         5|     Liam|    Brown|05/03/1987|     Sales| 85000|\n",
      "|         6|      Ava|   Garcia|22/04/1995|     Other| 82000|\n",
      "|         7|  William| Martinez|10/02/1981|        IT| 77000|\n",
      "|         8|   Sophia| Robinson|12/09/1988| Marketing| 94000|\n",
      "|         9|    James|    Clark|19/06/1982|     Other| 81000|\n",
      "|        10|Charlotte|Rodriguez|08/07/1991|        HR| 88000|\n",
      "|        11| Benjamin|    Lewis|30/11/1983|     Sales| 95000|\n",
      "|        12|      Mia|      Lee|17/02/1986| Marketing| 78000|\n",
      "|        13|    Ethan|   Walker|25/08/1992|     Other| 92000|\n",
      "|        14|   Harper|     Hall|31/05/1993|     Sales| 89000|\n",
      "|        15|     Noah|    Allen|12/01/1984|        IT| 76000|\n",
      "|        16|   Amelia|    Young|13/02/1994| Marketing| 93000|\n",
      "|        17|    Jacob|Hernandez|27/04/1987|     Sales| 91000|\n",
      "|        18| Isabella|     King|03/12/1992|     Other| 74000|\n",
      "|        19|     Luke|   Wright|22/01/1985|        IT| 86000|\n",
      "|        20|   Evelyn|    Lopez|05/03/1990| Marketing| 83000|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------- DATA Transformation ------------\n",
    "'''\n",
    "Any employee record with an empty value in the 'Department' column should\n",
    "be assigned to a department named 'Other'\n",
    "'''\n",
    "employees_df = employees_df.na.fill(\"Other\", [\"Department\"])\n",
    "\n",
    "#Updated Employees Table \n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb61c763-4ca5-4271-ac9d-cc9bf6ab2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- DATA Manipulation ----------\n",
    "\n",
    "#Get total salaries\n",
    "total_salaries = employees_df.agg(spark_sum(\"Salary\")).first()[0]\n",
    "\n",
    "# Get Total Salaries per department \n",
    "department_salaries = (\n",
    "    employees_df.groupBy(\"Department\")\n",
    "    .agg(spark_sum(\"Salary\").alias(\"Total_Salary\"))\n",
    "    .sort(col(\"Department\"))\n",
    ")\n",
    "\n",
    "# Get Top 5 highest paid employees in the company  \n",
    "top_paid_employees = employees_df.sort(desc(\"Salary\")).limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "92e8df1f-6c5f-44c7-82cc-db0223697640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Salaries Sum of employees: 1679000\n",
      "\n",
      "Total Salaries Per department: \n",
      "\n",
      "+----------+------------+\n",
      "|Department|Total_Salary|\n",
      "+----------+------------+\n",
      "|        HR|      163000|\n",
      "|        IT|      329000|\n",
      "| Marketing|      428000|\n",
      "|     Other|      329000|\n",
      "|     Sales|      430000|\n",
      "+----------+------------+\n",
      "\n",
      "\n",
      "Top 5 Paid employees: \n",
      "\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "|EmployeeID|FirstName| LastName| BirthDate|Department|Salary|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "|        11| Benjamin|    Lewis|30/11/1983|     Sales| 95000|\n",
      "|         8|   Sophia| Robinson|12/09/1988| Marketing| 94000|\n",
      "|        16|   Amelia|    Young|13/02/1994| Marketing| 93000|\n",
      "|        13|    Ethan|   Walker|25/08/1992|     Other| 92000|\n",
      "|        17|    Jacob|Hernandez|27/04/1987|     Sales| 91000|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#------- DATA Vizualisation ----------\n",
    "\n",
    "print('Total Salaries Sum of employees:',total_salaries)\n",
    "\n",
    "print('\\nTotal Salaries Per department: \\n')\n",
    "department_salaries.show()\n",
    "\n",
    "print('\\nTop 5 Paid employees: \\n')\n",
    "top_paid_employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdd773-74b2-49c9-9f3d-754d0c49e90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6cc75be0-d36b-4058-9741-e26cb90003ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `employees` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Bonus: Using Spark SQL\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mTo use sql query syntax in pyspark, first we need to convert the spark dataframe \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03minto a temporary view which will allow us to reference it during SQL. \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43memployees_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memployees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:370\u001b[0m, in \u001b[0;36mDataFrame.createTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a local temporary view with this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary table is tied to the :class:`SparkSession`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m \n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `employees` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views."
     ]
    }
   ],
   "source": [
    "# Bonus: Using Spark SQL\n",
    "\n",
    "'''\n",
    "To use sql query syntax in pyspark, first we need to convert the spark dataframe \n",
    "into a temporary view which will allow us to reference it during SQL. \n",
    "'''\n",
    "employees_df.createTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3f527e89-2941-4e6e-b2f6-d80f90d29857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|total_salary|\n",
      "+------------+\n",
      "|     1679000|\n",
      "+------------+\n",
      "\n",
      "+----------+------------+\n",
      "|Department|Total_Salary|\n",
      "+----------+------------+\n",
      "|        HR|      163000|\n",
      "|        IT|      329000|\n",
      "| Marketing|      428000|\n",
      "|     Other|      329000|\n",
      "|     Sales|      430000|\n",
      "+----------+------------+\n",
      "\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "|EmployeeID|FirstName| LastName| BirthDate|Department|Salary|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "|        11| Benjamin|    Lewis|30/11/1983|     Sales| 95000|\n",
      "|         8|   Sophia| Robinson|12/09/1988| Marketing| 94000|\n",
      "|        16|   Amelia|    Young|13/02/1994| Marketing| 93000|\n",
      "|        13|    Ethan|   Walker|25/08/1992|     Other| 92000|\n",
      "|        17|    Jacob|Hernandez|27/04/1987|     Sales| 91000|\n",
      "+----------+---------+---------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total Salaries Sum \n",
    "total_salaries_sql = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT SUM(Salary) AS total_salary\n",
    "    FROM employees\n",
    "    \"\"\"\n",
    ")\n",
    "department_salaries_sql = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT Department, SUM(Salary) AS Total_Salary\n",
    "    FROM employees\n",
    "    GROUP BY Department\n",
    "    ORDER BY Department\n",
    "    \"\"\"\n",
    ")\n",
    "top_paid_employees_sql = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM employees\n",
    "    ORDER BY Salary DESC\n",
    "    LIMIT 5;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "total_salaries_sql.show()\n",
    "\n",
    "department_salaries_sql.show()\n",
    "\n",
    "top_paid_employees_sql.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c3433-b764-4c3a-a1a8-ba4d87b6f9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
